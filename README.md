# Nomous

<div align="center">
  <img src="logo.png" alt="Nomous Logo" width="800"/>
  <br>
  <strong>A Local Autonomy Runtime Bridge</strong>
</div>

## Overview
Nomous is an offline-first WebSocket bridge that connects a TypeScript React dashboard to local AI services. The runtime wires together:
- ğŸ§  **Local LLM** loading via `llama.cpp`, including live load progress reporting and runtime sampling controlsã€F:src/backend/llm.pyâ€ L21-L115ã€‘ã€F:src/frontend/App.tsxâ€ L330-L401ã€‘
- ğŸ¤ **Speech-to-Text** streaming from the browser microphone to Vosk with configurable sensitivity and automatic LLM triggeringã€F:src/backend/audio.pyâ€ L15-L118ã€‘ã€F:scripts/run_bridge.pyâ€ L108-L158ã€‘
- ğŸ—£ï¸ **Text-to-Speech** synthesis through Piper with runtime toggles, voice switching, and optional auto playbackã€F:src/backend/tts.pyâ€ L15-L138ã€‘ã€F:scripts/run_bridge.pyâ€ L72-L157ã€‘
- ğŸ‘ï¸ **Computer Vision** capture with MediaPipe gesture detection, motion-aware frame streaming, and autonomous vision prompts for the LLMã€F:src/backend/video.pyâ€ L1-L176ã€‘ã€F:src/backend/video.pyâ€ L360-L437ã€‘
- ğŸ“ˆ **System telemetry** that surfaces CPU/GPU utilisation and memory pressure inside the dashboard UIã€F:src/backend/system.pyâ€ L1-L152ã€‘ã€F:src/frontend/components/SystemUsageCard.tsxâ€ L1-L83ã€‘
- ğŸ“š **Persistent memory graph & tool system** that lets the model store observations, run self-evaluations, and surface tool activity in the UIã€F:src/backend/memory.pyâ€ L1-L170ã€‘ã€F:src/backend/tools.pyâ€ L1-L146ã€‘ã€F:src/frontend/App.tsxâ€ L376-L430ã€‘

## Feature Highlights
- âœ¨ Real-time token streaming, load overlays, and chat playback for every assistant responseã€F:src/backend/llm.pyâ€ L116-L212ã€‘ã€F:src/frontend/App.tsxâ€ L330-L401ã€‘
- ğŸ› ï¸ **Nine built-in LLM tools** covering memory search, observation logging, learning, and social safeguards with UI visualisation of tool usage and statsã€F:src/backend/tools.pyâ€ L32-L187ã€‘ã€F:src/frontend/components/ToolActivity.tsxâ€ L6-L213ã€‘
- ğŸ“¹ Live camera preview with motion thresholds, gesture detection, and configurable debounce/cooldown windowsã€F:src/backend/video.pyâ€ L28-L123ã€‘ã€F:src/backend/video.pyâ€ L360-L437ã€‘
- ğŸ™ï¸ Browser microphone streaming (PCM16) with automatic speech detection and configurable sensitivityã€F:src/backend/audio.pyâ€ L33-L118ã€‘
- ğŸ”Š Optional Piper TTS synthesis with per-session toggles for enablement, auto playback, volume, and voice selectionã€F:src/backend/tts.pyâ€ L19-L138ã€‘ã€F:scripts/run_bridge.pyâ€ L72-L154ã€‘
- ğŸ§  Persistent memory graph with SQLite backing plus front-end visualisation and analytics helpersã€F:src/backend/memory.pyâ€ L13-L167ã€‘ã€F:src/frontend/App.tsxâ€ L403-L430ã€‘
- ğŸ“Š Dashboard telemetry (system metrics, behaviour scoring, reward accounting, token throughput) surfaced through dedicated cards and chartsã€F:src/backend/handlers.pyâ€ L24-L43ã€‘ã€F:src/frontend/components/SystemUsageCard.tsxâ€ L1-L83ã€‘ã€F:src/frontend/App.tsxâ€ L1614-L2158ã€‘

## Feature Status
### âœ… Complete & Working
- Local llama.cpp LLM loading with progress events, sampling controls, and autonomous thought/vision loopsã€F:src/backend/llm.pyâ€ L21-L212ã€‘ã€F:scripts/run_bridge.pyâ€ L108-L213ã€‘
- Vosk-based microphone pipeline that feeds transcripted speech into the LLM and echoes microphone activity back to the UIã€F:src/backend/audio.pyâ€ L33-L118ã€‘ã€F:src/frontend/App.tsxâ€ L330-L383ã€‘
- MediaPipe-enabled camera loop with motion gating, gesture detection, and asynchronous vision prompts for the modelã€F:src/backend/video.pyâ€ L28-L123ã€‘ã€F:src/backend/video.pyâ€ L360-L437ã€‘
- Tool execution framework with nine tools, execution history, and dedicated dashboard widgetsã€F:src/backend/tools.pyâ€ L32-L187ã€‘ã€F:src/frontend/components/ToolActivity.tsxâ€ L6-L213ã€‘
- System telemetry publishing CPU/GPU metrics and surfacing device context in the UI system usage cardã€F:src/backend/system.pyâ€ L1-L152ã€‘ã€F:src/frontend/components/SystemUsageCard.tsxâ€ L1-L83ã€‘
- Persistent memory store that records interactions and streams the graph to the UI via WebSocket eventsã€F:src/backend/memory.pyâ€ L13-L167ã€‘ã€F:src/frontend/App.tsxâ€ L403-L430ã€‘

### âš ï¸ Work in Progress / Limited Support
- Piper auto-playback relies on a Windows PowerShell media player shim; audio playback must be replaced for Linux/macOS deployments even though synthesis still worksã€F:src/backend/tts.pyâ€ L87-L123ã€‘
- GPU metrics require `torch`, `pynvml`, and `psutil`; missing dependencies will disable telemetry and cause the Python test suite to fail when system monitoring tests import themã€F:src/backend/system.pyâ€ L1-L152ã€‘ã€F:tests/test_system.pyâ€ L1-L120ã€‘ã€62a53aâ€ L1-L36ã€‘
- Automated tooling around pytest async marks needs configuration â€“ warnings appear until a custom `asyncio` marker is registered in `pytest.ini`ã€F:tests/test_tools.pyâ€ L80-L328ã€‘ã€62a53aâ€ L11-L36ã€‘

### ğŸš§ Not Yet Implemented / Planned
- Additional gesture classifiers (thumbs down, fists, etc.) and configurable gesture-to-action mappings noted in the roadmapã€F:docs/CHANGELOG.mdâ€ L136-L170ã€‘
- Custom wake-word activation for hands-free microphone captureã€F:docs/CHANGELOG.mdâ€ L168-L170ã€‘
- Cross-platform audio playback backend for Piper-generated speech so Linux/macOS users receive spoken feedback without external toolsã€F:src/backend/tts.pyâ€ L87-L123ã€‘

## Quick Start

### One-command bootstrap
Run the launcher to automatically create a virtual environment, install Python
and Node dependencies, validate required tooling, and start both the backend and
frontend services:

```bash
python run_nomous.py
# Legacy invocation supported for convenience
python start.py
```

If any prerequisite is missing or a step fails, the launcher prints a detailed
message explaining what happened and how to resolve it.

### Manual setup
1. Clone the repository
```bash
git clone https://github.com/ant3869/nomous.git
cd nomous
```

2. Install dependencies
```bash
# Backend
pip install -r requirements.txt

# Frontend
npm install
```

3. Configure `config.yaml` with your model paths and settings

4. Start the full bridge (backend workers + React dev server)
```bash
python scripts/start.py
```

To run only the backend bridge without launching the UI dev server:
```bash
python scripts/run_bridge.py
```

## Project Structure
```
nomous/
â”œâ”€ src/
â”‚  â”œâ”€ backend/         # Python WebSocket server
â”‚  â”‚  â”œâ”€ audio.py     # Audio processing
â”‚  â”‚  â”œâ”€ video.py     # Video capture
â”‚  â”‚  â”œâ”€ llm.py       # LLM integration
â”‚  â”‚  â””â”€ tts.py       # Text-to-speech
â”‚  â”‚
â”‚  â””â”€ frontend/       # React TypeScript UI
â”‚     â”œâ”€ components/  # UI components
â”‚     â””â”€ App.tsx      # Main application
â”‚
â”œâ”€ scripts/           # Utility scripts
â”œâ”€ tests/            # Test suite
â””â”€ config.yaml       # Configuration
```

## Configuration
Key configuration sections in `config.yaml`:

```yaml
paths:
  gguf_path: path/to/model.gguf
  embed_gguf_path: models/embed/bge-small-en-v1.5-f16.gguf
  vosk_model_dir: path/to/vosk-model
  piper_exe: path/to/piper.exe
  piper_voice: path/to/voice.onnx
  piper_out_dir: path/to/output

llm:
  enable: true
  n_ctx: 2048
  n_threads: 4
  temperature: 0.6
  top_p: 0.95
```

See [INSTALL_INSTRUCTIONS.md](docs/INSTALL_INSTRUCTIONS.md) for detailed setup steps.

### Optional Components
- **GPU acceleration** â€“ enable by setting `llm.n_gpu_layers` and installing CUDA-enabled `llama-cpp-python`; telemetry additionally needs `torch` + `pynvml` (see `requirements.txt`).ã€F:src/backend/llm.pyâ€ L89-L115ã€‘ã€F:src/backend/system.pyâ€ L1-L152ã€‘
- **Piper text-to-speech** â€“ requires `paths.piper_exe`, `paths.piper_voice`, and `paths.piper_out_dir` to exist; auto playback currently targets Windows PowerShell.ã€F:src/backend/tts.pyâ€ L29-L138ã€‘
- **Vision analysis & gestures** â€“ ensure OpenCV can access your camera and install `mediapipe` for gesture detection.ã€F:src/backend/video.pyâ€ L15-L68ã€‘

## Testing
Run the test suite:
```bash
python -m pytest tests/
```

## Documentation
- [Installation Guide](docs/INSTALL_INSTRUCTIONS.md)
- [Frontend Documentation](docs/README_FE.md)
- [LLM Tool System](docs/TOOLS.md)
- [Implementation Summary](docs/IMPLEMENTATION_SUMMARY.md)
- [Changelog](docs/CHANGELOG.md)
- [Bug Fixes](docs/BUG_FIXES.md)
- [Testing Guide](docs/TESTING.md)

## GitHub Copilot Collections
The following Copilot collections are installed to enhance development:

### Python Development & ML Collection
- ğŸ“ **Instructions**: GPU optimization, ML model integration, async patterns
- ğŸ¯ **Prompts**: GPU optimization guide
- ğŸ’­ **Chat Mode**: Python ML expert for guidance

### Frontend Web Development Collection
- ğŸ“ **Instructions**: React, TypeScript, WebSocket best practices
- ğŸ¯ **Prompts**: React component generator
- ğŸ’­ **Chat Mode**: Frontend expert for assistance

### Testing & Test Automation Collection
- ğŸ“ **Instructions**: WebSocket and ML model testing patterns
- ğŸ¯ **Prompts**: Test suite generator
- ğŸ’­ **Chat Mode**: Testing expert for guidance

### Security & Code Quality Collection
- ğŸ“ **Instructions**: Security best practices and code quality
- ğŸ¯ **Prompts**: Code quality enhancer
- ğŸ’­ **Chat Mode**: Security & quality expert

The collections are located in `.github/copilot/` and include:
```
.github/copilot/
â”œâ”€ instructions/    # Best practices and guidelines
â”œâ”€ prompts/        # Task-specific generators
â””â”€ chatmodes/      # Expert chat modes
```

## Technical Stack
- **Backend**: Python, asyncio, websockets, OpenCV, Vosk, Piper
- **Frontend**: React 18+, TypeScript, Tailwind CSS, shadcn/ui
- **ML**: llama.cpp, GGUF models, BGE embeddings
- **Build**: Vite, PostCSS

## License
MIT License - See [LICENSE](LICENSE) for details